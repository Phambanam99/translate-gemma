# ============================================================================
# TranslateGemma Backend Environment Configuration
# ============================================================================
# Copy this file to .env and configure values for your environment

# ============================================================================
# GPU Configuration (IMPORTANT for Docker)
# ============================================================================

# GPU Device ID (0, 1, 2, 3, etc.)
# Choose which GPU to use. Your system has 4 A100 GPUs (0-3)
# GPU 2 has the most available memory (~79GB free)
GPU_DEVICE_ID=2

# CUDA Visible Devices (must match GPU_DEVICE_ID)
# This controls which GPUs are visible to the application
# Use single GPU: "2"
# Use multiple GPUs: "0,1,2,3"
CUDA_VISIBLE_DEVICES=2

# CUDA Launch Blocking (helps with debugging, but slower)
# Default: 0 (disabled)
# Set to 1 for debugging CUDA errors
CUDA_LAUNCH_BLOCKING=0

# ============================================================================
# Model Cache Configuration
# ============================================================================

# Hugging Face cache directory for models
# Default: ~/.cache/huggingface (on host) or /app/model_cache (in Docker)
TRANSFORMERS_CACHE=/app/model_cache
HF_HOME=/app/model_cache

# Hugging Face API Token (for gated models)
# Leave empty if using public models only
HF_TOKEN=

# ============================================================================
# Offline Mode (for air-gapped environments)
# ============================================================================

# Enable offline mode (no internet required)
# Default: false
HF_HUB_OFFLINE=false
TRANSFORMERS_OFFLINE=false

# ============================================================================
# API Server Configuration
# ============================================================================

# Server host and port
API_HOST=0.0.0.0
API_PORT=8000

# ============================================================================
# Performance Tuning
# ============================================================================

# CSV preprocessing settings
CSV_CPU_WORKERS=8
CSV_PREPROCESS_CHUNK_SIZE=1000
CSV_GPU_BATCH_SIZE=20

# Max tokens per generation (adjust based on your GPU memory)
MAX_NEW_TOKENS=512

# ============================================================================
# Environment Type
# ============================================================================

# production, development
ENVIRONMENT=production
DEBUG=false
